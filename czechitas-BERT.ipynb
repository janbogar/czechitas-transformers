{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janbogar/czechitas-transformers/blob/main/czechitas-BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COdcar5UEot5"
      },
      "source": [
        "# Transformers for text processing\n",
        "\n",
        "Transformers are neural networks that changed how we model sequential data.\n",
        "\n",
        "There are many kinds of data that are sequential in nature, for example time series, but e.g. a text is also just a sequence of words.\n",
        "\n",
        "State of the art in these areas used to be recurrent neural networks. They work by sequentially taking as an input not just each item in the sequence, but also the output of the network on the previous item. This way, the output of the network can contain the memory of what happened previously.\n",
        "\n",
        "The problem with recurrent neural networks was difficulty of training them in parallel. They also had trouble learning relationships between words that were far away from each other.\n",
        "\n",
        "Transformers work quite differently. All of the sequence is shown to the network at once, and a special architecture called attention head (transformers usually have multiple) predict to which parts of sentence should the network pay attention when processing each word.\n",
        "\n",
        "Famous language models based on transformers are e.g. BERT and GPT3, but there are many more variants, such as RoBERTa, ALBERT, sBERT or ELECTRA.\n",
        "\n",
        "# Model\n",
        "In this notebook we will use BERT model trained and published by researches at West Bohemia University. The model is called Czert and the authors experimented with it on many tasks:\n",
        " - Sequence Classification (**Sentiment Classification**, Multi-label Document Classification)\n",
        " - Sequence Pair Classification (Semantic Text Similarity)\n",
        " - Token Classification (Morphological Tagging, Named Entity Recognition,\n",
        "Semantic Role Labeling)\n",
        "\n",
        "The model:\n",
        "https://huggingface.co/UWB-AIR/Czert-B-base-cased\n",
        "\n",
        "The paper:\n",
        "https://arxiv.org/pdf/2103.13031.pdf\n",
        "\n",
        "# Sentiment analysis\n",
        "\n",
        "In this notebook, we will use the model trained specifically for sentiment analysis. In the sentiment analysis task, the model attempts to classify short texts (often social media statuses or customer reviews) as either positive, negative or neutral.\n",
        "\n",
        "The authors published two versions of the model, one trained on reviews from www.csfd.cz and the other trained on facebook posts. Both datasets are available on their website.\n",
        "\n",
        "The authors claim, that this model had F1 score of 77% on CSFD data and 85% percent on FB data, which they claim are better than the state of the art before this model was published.\n",
        "F1 score is hard to interpret without knowledge of precision and recall metrics, but for now, let's just say it corresponds somehow to the accuracy of the model.\n",
        "\n",
        "If the model had 75% accuracy, it would still make an error in one quarter of cases. That means that there is still a huge potential for error.\n",
        "This is something to keep in mind whenever you are deciding on a specific model: what is the minimal acceptable accuracy for my use-case?\n",
        "\n",
        "In the following cells, we will download this model and attempt to use it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbEbmQinG2OY"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJMC7riyHlHt"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TIoBmL3G72S"
      },
      "source": [
        "**!! Restart runtime after installing !!**\n",
        "\n",
        "Go to the top of the page, press 'Runtime\" and select \"Restart runtime\".\n",
        "\n",
        "## Download model\n",
        "There are two variants of the model, we will download the one trained on FB data and unzip it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i4GED7la5y2",
        "outputId": "b3bf1ec5-9fef-4cd1-f84e-8be5eb1da502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-12-05 12:11:00--  https://air.kiv.zcu.cz/public/CZERT-B_fb.zip\n",
            "Resolving air.kiv.zcu.cz (air.kiv.zcu.cz)... 147.228.63.35\n",
            "Connecting to air.kiv.zcu.cz (air.kiv.zcu.cz)|147.228.63.35|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 406510308 (388M) [application/zip]\n",
            "Saving to: ‘CZERT-B_fb.zip.2’\n",
            "\n",
            "CZERT-B_fb.zip.2    100%[===================>] 387.68M  7.30MB/s    in 59s     \n",
            "\n",
            "2022-12-05 12:12:00 (6.56 MB/s) - ‘CZERT-B_fb.zip.2’ saved [406510308/406510308]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#!wget https://air.kiv.zcu.cz/public/CZERT-B_csfd.zip\n",
        "!wget https://air.kiv.zcu.cz/public/CZERT-B_fb.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygk6MRCIa--G",
        "outputId": "47342162-3c69-4749-fd0c-2198730345f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  CZERT-B_fb.zip\n",
            "  inflating: CZERT-B_fb_BS-32_EC-12_LR-0-0000020_LEN-64_SCH-linear_wrp_CPU-True_TRAIN-True_CUST_LAYER-False_BINARY-False_2020-11-30_13-00_40-335326_F1-0.7598/config.json  \n",
            "  inflating: CZERT-B_fb_BS-32_EC-12_LR-0-0000020_LEN-64_SCH-linear_wrp_CPU-True_TRAIN-True_CUST_LAYER-False_BINARY-False_2020-11-30_13-00_40-335326_F1-0.7598/pytorch_model.bin  \n",
            "  inflating: CZERT-B_fb_BS-32_EC-12_LR-0-0000020_LEN-64_SCH-linear_wrp_CPU-True_TRAIN-True_CUST_LAYER-False_BINARY-False_2020-11-30_13-00_40-335326_F1-0.7598/special_tokens_map.json  \n",
            "  inflating: CZERT-B_fb_BS-32_EC-12_LR-0-0000020_LEN-64_SCH-linear_wrp_CPU-True_TRAIN-True_CUST_LAYER-False_BINARY-False_2020-11-30_13-00_40-335326_F1-0.7598/tokenizer_config.json  \n",
            "  inflating: CZERT-B_fb_BS-32_EC-12_LR-0-0000020_LEN-64_SCH-linear_wrp_CPU-True_TRAIN-True_CUST_LAYER-False_BINARY-False_2020-11-30_13-00_40-335326_F1-0.7598/vocab.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip -o CZERT-B_fb.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e423bsPjKsZm"
      },
      "source": [
        "# Load the model\n",
        "This model is implemented as a HuggingFace transformer. \n",
        "This means they have a very convenient API and can be easily loaded and reused.\n",
        "I highly recommend their course at:\n",
        "https://huggingface.co/course/chapter1/1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "vyrXlWYEHyHb"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, BertForSequenceClassification\n",
        "import tensorflow\n",
        "\n",
        "#model_path=\"CZERT-B_csfd_BS-14_EC-12_LR-0-0000200_LEN-512_SCH-linear_wrp_CPU-False_TRAIN-True_CUST_LAYER-False_BINARY-False_2020-12-06_10-30_07-494882_F1-0.8467/\"\n",
        "model_path=\"CZERT-B_fb_BS-32_EC-12_LR-0-0000020_LEN-64_SCH-linear_wrp_CPU-True_TRAIN-True_CUST_LAYER-False_BINARY-False_2020-11-30_13-00_40-335326_F1-0.7598\"\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXDOi4ZOLTay"
      },
      "source": [
        "## What is a tokenizer\n",
        "\n",
        "Neural networks accept as an input vectors or matrices of numbers, so we need to turn texts that we want to input into the model into vectors.\n",
        "\n",
        "Tokenizer is a program that does that. It splits the data into tokens and then assigns each token a number.\n",
        "There is no universally optimal way to tokenize text and many approaches can be used, depending on the specific use case.\n",
        "Transformers usually use sub-word tokenization. Here is an example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C29Q_Bn3LSnu",
        "outputId": "85ff4575-2aba-42e1-d1c5-2e5ef16deb64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens: '[CLS]' 'za' 'hory' ',' 'za' 'dol' '##y' ',' 'me' 'zlat' '##e' 'par' '##oh' '##y' '[SEP]'\n",
            "encoding:  [2, 1967, 12378, 16, 1967, 10864, 1036, 16, 2913, 7218, 1011, 2667, 2596, 1036, 3]\n"
          ]
        }
      ],
      "source": [
        "sentence = \"Za hory, za doly, mé zlaté parohy\"\n",
        "sentence_tokens = [\"[CLS]\"] + tokenizer.tokenize(sentence) + [\"[SEP]\"]\n",
        "sentence_encoding=tokenizer.encode(sentence)\n",
        "print(\"tokens:\" ,\" \".join([f\"\\'{i}\\'\" for i in sentence_tokens]))   \n",
        "print(\"encoding: \",sentence_encoding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZJQL6rHMe7o"
      },
      "source": [
        "The tokenizer split some words into multiple tokens and added special tokens and marks:\n",
        " - `[CLS]` token is always added at the start of the text. Transformer outputs one vector per each token, and the vector for `[CLS]` token is used for the classification of whole sentences.\n",
        "\n",
        "- `[SEP]` token: transformers are also trained on tasks including relationship of multiple texts. In that case, texts are separated by this special token.\n",
        "\n",
        "- `[PAD]` token was not used in this case, but if there is a need to make multiple sequences the same length, the unused space is filled with `[PAD]` tokens.\n",
        "\n",
        "- `##oh` token: the word `parohy` was split into tokens `par`,`oh` and `y`. To signal to the model that the tokens `oh` and `y` were not at the start of the word, they are prepended with `##`.\n",
        "\n",
        "After splitting the text and adding special tokens, every token is turned into a number, based on its position in the vocabulary- the document containing all tokens known to the model. If some token is not in the vocabulary, it is replaced with a special token `[UNK]`. The vocabulary file was downloaded together with the model, and in this case contains one word per line, so if we want to see how many words it contains, we just count the lines in it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uOVoCLgPBur",
        "outputId": "438510ea-cd6e-4436-f8a0-5f70b07294c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30000 CZERT-B_fb_BS-32_EC-12_LR-0-0000020_LEN-64_SCH-linear_wrp_CPU-True_TRAIN-True_CUST_LAYER-False_BINARY-False_2020-11-30_13-00_40-335326_F1-0.7598/vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!wc -l CZERT-B_fb_BS-32_EC-12_LR-0-0000020_LEN-64_SCH-linear_wrp_CPU-True_TRAIN-True_CUST_LAYER-False_BINARY-False_2020-11-30_13-00_40-335326_F1-0.7598/vocab.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xaZy7fEPP-E"
      },
      "source": [
        "Here are the last 10 tokens in the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGXt5PTpPJKK",
        "outputId": "68eba2f7-6191-4f7f-df5a-d240f4a046d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "záchranář\n",
            "zahrála\n",
            "repr\n",
            "Britská\n",
            "stěže\n",
            "izp\n",
            "řešíme\n",
            "##váří\n",
            "##natý\n",
            "Jerem\n"
          ]
        }
      ],
      "source": [
        "! tail -n 10 CZERT-B_fb_BS-32_EC-12_LR-0-0000020_LEN-64_SCH-linear_wrp_CPU-True_TRAIN-True_CUST_LAYER-False_BINARY-False_2020-11-30_13-00_40-335326_F1-0.7598/vocab.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56YW0johPDa5"
      },
      "source": [
        "# Using the model\n",
        "\n",
        "Now that we know how to preprocess the input texts, let's try the model on some examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "964a4mMdTxTj",
        "outputId": "66ed8451-1604-493e-d1dd-c04e97a7e578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw sentences:\n",
            " ['Pivo je skvělý.', 'Pivo příšerně chutná, je nezdravé a je vstupnou bránou k alkoholizmu. Jednoduše fuj.', 'Dávam tomuhle pivu 50 procent.']\n",
            "\n",
            "Input of the model after tokenization:\n",
            " {'input_ids': tensor([[    2, 10671,  1955, 15584,  3312,    18,     3,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    2, 10671, 10769,  9930,  2121, 24056,  1999,    16,  1955, 16840,\n",
            "          2059,  1011,    69,  1955,  4073,  2104,  4181,  1929,    79, 27696,\n",
            "          1021,  2028,    18,  4234,  2349, 11293,  1026,    18,     3],\n",
            "        [    2, 10203,  2058,  2555,  3045,  8828,  1025,  3784,  2662,    18,\n",
            "             3,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0]])}\n",
            "\n",
            "Output of the model:\n",
            " SequenceClassifierOutput(loss=None, logits=tensor([[-2.2283,  3.5561, -0.5077],\n",
            "        [ 3.0172, -1.6817, -1.5079],\n",
            "        [-0.4129, -0.8393,  1.8872]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n"
          ]
        }
      ],
      "source": [
        "test_strings = [\"Pivo je skvělý.\",\"Pivo příšerně chutná, je nezdravé a je vstupnou bránou k alkoholizmu. Jednoduše fuj.\",\"Dávam tomuhle pivu 50 procent.\"]\n",
        "\n",
        "print(\"Raw sentences:\\n\",test_strings)\n",
        "tokenized_inputs = tokenizer(test_strings, return_tensors=\"pt\", padding=True)\n",
        "print(\"\\nInput of the model after tokenization:\\n\", tokenized_inputs)\n",
        "\n",
        "outputs = model(**tokenized_inputs)\n",
        "print(\"\\nOutput of the model:\\n\", outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1boirDMPous"
      },
      "source": [
        "As we can see, the output of the model contains a matrix of numbers called logits.\n",
        "Logits are numbers between minus infinity and  plus infinity that represent probability. They can be turned into probability distribution (i.e. a set of numbers between 0 and 1 that together add up to 1) by softmax function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE2ch4Pei4Zm",
        "outputId": "b81a5afa-f1b3-4e0d-cebc-2b43e4eb8582"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-66-45e692d90b33>:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  softmax(outputs.logits)  #turn into probabilities (so they are between 0 and 1 and sum into 1)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[0.0030, 0.9801, 0.0168],\n",
              "        [0.9805, 0.0089, 0.0106],\n",
              "        [0.0860, 0.0561, 0.8579]], grad_fn=<SoftmaxBackward>)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.nn.functional import softmax\n",
        "softmax(outputs.logits)  #turn into probabilities (so they are between 0 and 1 and sum into 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBzXV_p0Rz70"
      },
      "source": [
        "Each row in this matrix is prediction for one input text. There are three numbers for each text:\n",
        " - probability of the text being negative\n",
        " - probability of the text being positive\n",
        " - probability of the text being neutral\n",
        "\n",
        "You can check yourself that they add to 1.\n",
        "\n",
        "\n",
        "In the next cells, we gather all of this code into one function, which will accept a list of texts and will make a prediction for each one.\n",
        "I also wrote a function that will print the prediction nicely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "QbK7Ao-WeQF_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "id2label={0:\"negative\",1:\"positive\",2:\"neutral\"}\n",
        "label2id={v:k for k,v in id2label.items()}\n",
        "\n",
        "def get_predictions(texts):\n",
        "  \"\"\"Function that accepts a list of texts and outputs an iterator of dictionaries with predictions.\"\"\"\n",
        "  for text in texts:\n",
        "    tokens=tokenizer(text, return_tensors=\"pt\",truncation=\"only_first\",max_length=512)\n",
        "    model_output=model(**tokens)\n",
        "    probabilities=softmax(model_output.logits[0],dim=0).detach().numpy()\n",
        "    highest_probability=np.argmax(probabilities)\n",
        "    yield {\"text\":text,                                                          # input text\n",
        "           \"probabilities\":{k:probabilities[v] for k,v in label2id.items()},     # probabilities of each class (negative,positive and neutral)\n",
        "           \"prediction\":id2label[highest_probability],                           # prediction: the label of the class with highest probability\n",
        "           \"prediction_confidence\":probabilities[highest_probability]}           # prediction confidence: the probability of the class with the highest probability\n",
        "\n",
        "def print_predictions(predictions):\n",
        "  \"\"\"Function that prettyprints predictions produced by get_prediction function.\"\"\"\n",
        "  for i in predictions:\n",
        "    print(f\"{i['prediction']} ({i['prediction_confidence']:.2f}): \\\"{i['text']}\\\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImBe0q5JluG9",
        "outputId": "14ab3251-d23f-448b-e718-ec5687b0cf23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "positive (0.93): \"tohle je skvele, nejradeji bych to delal cely den\"\n",
            "negative (0.98): \"je to hrozna otrava, nejradeji bych to zakazal\"\n",
            "neutral (0.70): \"davam 50 procent, neurazi\"\n"
          ]
        }
      ],
      "source": [
        "example_texts=[\"tohle je skvele, nejradeji bych to delal cely den\",\"je to hrozna otrava, nejradeji bych to zakazal\",\"davam 50 procent, neurazi\"]\n",
        "\n",
        "print_predictions(get_predictions(example_texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GejyBob2mv-k"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Use the code in the previous cell to find sentences that will fool the model in these ways:\n",
        " - the model will output negative if the sentence is positive\n",
        " - the model will output positive if the sentence is negative\n",
        " - the model will output neutral if the sentence is either positive or negative\n",
        " - the model will output positive or negative if the sentence is neutral\n",
        "\n",
        "Which one is the hardest?\n",
        "\n",
        "Which is harder, to fool the model if the text is long or if it is short?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TFeIzKf7n5dc"
      },
      "outputs": [],
      "source": [
        "#do the exercise!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsW-iTmfXold"
      },
      "source": [
        "# Watching politicians\n",
        "\n",
        "Let's use this model for good: monitoring politicians.\n",
        "\n",
        "We can try to classify their posts on social media as positive, negative and neutral. Maybe that can tell us us something about their authors.\n",
        "\n",
        "# Where to get the data\n",
        "\n",
        "Luckily, there is a webpage called Hlidac Statu, which collected a huge dataset of social media interactions of czech politicians (and many other useful datasets).\n",
        "\n",
        "In the next few cells, we will write a function that will download this data.\n",
        "\n",
        "However to access it, you will need to register on www.hlidacstatu.cz\n",
        "To do so, follow these steps:\n",
        " - Go to https://www.hlidacstatu.cz/Identity/Account/Login?returnUrl=/\n",
        " - Login with your google account or register in other way\n",
        " - After logging in, click on your profile (top right corner)\n",
        " - In the menu on left, click the \"API klic\"\n",
        " - Copy the API key\n",
        " - Replace the string in the next code cell with your key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "Qvbtaaz_uGJj"
      },
      "outputs": [],
      "source": [
        "auth_key=\"<your API key>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWTe480AdXo_"
      },
      "source": [
        "This code uses the requests library to access the web API of Hlidac Statu and downloads one page of posts of Petr Fiala he made on Facebook.\n",
        "Pay attention to the params argument.\n",
        "\n",
        "We print out the first 5 texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "aqTMnxDkX31X"
      },
      "outputs": [],
      "source": [
        "import requests as rq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKRYgTxmuCUQ",
        "outputId": "43d6c778-ed64-4a57-f886-aa67821e9081"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['',\n",
              " 'Děkuji za jednomyslnou nominaci na předsedu ODS delegátům jihočeské ODS. Gratuluji Martinu Kubovi k jasnému obhájení pozice předsedy regionu. Jihočeská ODS získala v nedávných parlamentních volbách nadprůměrný výsledek. V Janu Zahradníkovi a nově i v Janu Bauerovi mají jihočeští voliči výborné zástupce v Poslanecké sněmovně, kteří dobře reprezentují naši politiku.',\n",
              " 'Dnes se snaží Listopad 89 využít ve svůj prospěch různí populisté, extremisté, komunisté a dokonce bývalí agenti StB. Nedovolme jim to. \\n\\nPřipomínejme si, co se tehdy skutečně dělo, mluvme s lidmi, kteří si tu dobu pamatují, kteří se revoluce aktivně účastnili. Proto pořádáme sérii diskuzí na mnoha místech naší země. \\n\\nRád se s Vámi uvidím například 9. října v Praze. https://www.ods.cz/30-let-svobody/setkani',\n",
              " '\"Letní interview\" s moderátorkou Michaelou Šmídovou. Neobvyklé prostředí s krásným výhledem na Prahu, ale mluvili jsme hodně o Brně a o Moravě. :-) Kromě politiky jsme si povídali třeba o italské kuchyni, tenisu, víně, slivovici, prostě o spoustě příjemných věcí. Můžete sledovat dnes večer ve 22:50 na TV Nova.',\n",
              " 'Lipová na Děčínsku se o víkendu stala letošní vesnicí roku 2019. Proto jsem dnes telefonicky pogratuloval jejímu starostovi Pavlu Svobodovi. \\n\\nPan starosta v posledních komunálních volbách vedl v Lipové kandidátku ODS, která zvítězila se ziskem takřka 60 % hlasů. Další důkaz, že to v Lipové dělají dobře. ;-)']"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response=rq.get(\"https://api.hlidacstatu.cz/Api/v2/datasety/vyjadreni-politiku/hledat\",params={'dotaz':\"server:Facebook AND osobaid:petr-fiala\",\"strana\":1},headers={\"Authorization\":auth_key})\n",
        "results=response.json()[\"results\"]\n",
        "texts=[i[\"text\"] for i in results]\n",
        "texts[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hFLoWJ3d9VN"
      },
      "source": [
        "To simplify our workflow, we wrap this code up as a function in the next cell.\n",
        "This function downloads 300 latest Facebook posts of a chosen politician that are in the database.\n",
        "The name of the politician must be lowercased and the spaces in it exchanged for a `-` sign (due to the design of the database).\n",
        "\n",
        "So \"andrej-babis\" instead of \"Andrej Babis\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "rk-DpUfpZxYk"
      },
      "outputs": [],
      "source": [
        "def get_page(name,page):\n",
        "  response=rq.get(\"https://api.hlidacstatu.cz/Api/v2/datasety/vyjadreni-politiku/hledat\",params={'dotaz':f\"server:Facebook AND osobaid:{name}\",\"strana\":{page},\"sort\":\"datum\",\"desc\":1},headers={\"Authorization\":auth_key})\n",
        "  return response.json()\n",
        "\n",
        "def get_texts(name,at_most=300):\n",
        "  \"\"\"\n",
        "  Download 300 newest posts of a politician in the database. Selects only posts made to Facebook.\n",
        "  \"\"\"\n",
        "  current_page=0\n",
        "  texts=[]\n",
        "  \n",
        "  page=get_page(name,current_page)\n",
        "  total_in_database=page[\"total\"]\n",
        "  print(\"total in database: \",total_in_database)\n",
        "\n",
        "  texts+=[i['text'] for i in page[\"results\"]]\n",
        "  print(f\"page: {current_page}, texts_so_far: {len(texts)}\")\n",
        "  \n",
        "  while not (len(texts)>=at_most or len(texts)>=total_in_database):\n",
        "    current_page+=1\n",
        "    page=get_page(name,current_page)\n",
        "    texts+=[i['text'] for i in page[\"results\"]]\n",
        "    print(f\"page: {current_page}, texts_so_far: {len(texts)}\")\n",
        "  \n",
        "  if len(texts)>at_most:\n",
        "    texts=texts[:at_most]\n",
        "  return texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmYKyLuGehyk"
      },
      "source": [
        "Let's download the posts of Andrej Babiš."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfAvUXttZyNA",
        "outputId": "97a01335-affe-43c6-e819-d8385f066675"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total in database:  10308\n",
            "page: 0, texts_so_far: 25\n",
            "page: 1, texts_so_far: 50\n",
            "page: 2, texts_so_far: 75\n",
            "page: 3, texts_so_far: 100\n",
            "page: 4, texts_so_far: 125\n",
            "page: 5, texts_so_far: 150\n",
            "page: 6, texts_so_far: 175\n",
            "page: 7, texts_so_far: 200\n",
            "page: 8, texts_so_far: 225\n",
            "page: 9, texts_so_far: 250\n",
            "page: 10, texts_so_far: 275\n",
            "page: 11, texts_so_far: 300\n"
          ]
        }
      ],
      "source": [
        "name=\"andrej-babis\"\n",
        "texts=get_texts(name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPKbsuK4fvi4"
      },
      "source": [
        "Now let's use the model to process these posts. It can take up to a few minutes, depending on the length of the posts. We than print the first five predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvWw2rio1t96",
        "outputId": "fa9f3d61-8432-4f79-e116-c752f3d65aa2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing texts: 100%|██████████| 300/300 [00:38<00:00,  7.75it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm   # tqdm is a python package for nice progress bars\n",
        "\n",
        "predictions=list(tqdm(get_predictions(texts),total=len(texts),desc=\"processing texts\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8pGiQJqhIxb",
        "outputId": "d48eecc9-36d8-4f44-f15f-c0e06ee1e75f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "positive (0.53): \"Je Nový rok. Velký svátek naší země a nás všech. A já mám něco na srdci ❤️\"\n",
            "neutral (0.98): \"Ode mě pro Vás. Můj neuvěřitelný rok 2021 ve fotkách. Třeba se tam najdete ❤️\"\n",
            "neutral (0.97): \"Kupuju obytňák!\"\n",
            "neutral (0.97): \"Kupuju obytňák!\"\n",
            "neutral (0.79): \"Je to tady. Kupuju obytňák!\"\n"
          ]
        }
      ],
      "source": [
        "print_predictions(predictions[:5])   # it seems the latest posts in the database are from the end of 2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubW54ntjf12h"
      },
      "source": [
        "In the next cell, let's take a look at probability distribution of confidences of predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "ezx9XXJX136L",
        "outputId": "746a759c-95aa-4c81-d832-f5720f684481"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x7f71a5c53c10>"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1080x360 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sb\n",
        "import pandas as pd\n",
        "sb.displot(pd.DataFrame(predictions),x=\"prediction_confidence\",col=\"prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQd-x23zib6k"
      },
      "source": [
        "Looks like the model is usually only confident about neutral predictions of Andrej Babis (most such predictions have high confidence) and much less confident about the positive and negative predictions.\n",
        "\n",
        "------\n",
        "## Question 1:\n",
        "\n",
        "Is this because of the data (positive and negative statements are written in relatively neutral way, compared to the training data), or is it because of the model (model is somehow biased towards neutral sentences)?\n",
        "\n",
        "How could we find out?\n",
        "\n",
        "#### Author's opinion:\n",
        "\n",
        "Without an experiment, this question cannot be answered - it is impossible to say whether something is an error of the model or a real feature of the data without a deeper look at both the model and the data.\n",
        "\n",
        "The best way to do this is to prepare our own testing dataset.\n",
        "What I would do:\n",
        " - Select 500 (or maybe even 1000) random statuses. They should be from different politicians and from different times (so we avoid them being all about the same topic that was important at the time of their writing)\n",
        " - Read each one and label it as either positive, negative or neutral. With 3s per post, it should take less than half an hour.\n",
        " - Use the model to make a prediction for each post.\n",
        " - Calculate the confusion matrix. Confusion matrix is a table with columns corresponding to predicted labels, and rows to the actual labels. Each cell then contains the number of datapoints with given actual label and given predicted label, relative to the total number of datapoints. Confusion matrix of the best possible model would have zeroes everywhere but on a diagonal. Values outside of the diagonal are probabilities of the model committing different types of errors (we already encountered different kinds of errors in the Exercise 1. Which cells in the confusion matrix corresponds to which kind of error?)). This alone should answer whether the model is biased towards classifying posts as neutral or not. Confusion matrix is one of the most valuable concepts for the model performance analysis.\n",
        " - Analyze some of the errors of the model. Print out 10 posts that were erroneously classified with the highest confidence (maybe do so for each error category, i.e. Neutral to Positive, Positive to negative, etc.) and read them. Is there some pattern? What kind of errors are these? This will help you understand what kind of errors is the model making and also set the threshold for confidence of the predictions (there are also more rigorous ways of doing this, but this is good for now).\n",
        "\n",
        "-------\n",
        "\n",
        "Let's say we would like to select the most extreme examples of politicians posts on the sentiment spectrum. In the next cell, we attempt to do this by selecting the positive/negative posts with the highest confidence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD7c7buU6zsl",
        "outputId": "1ff82b29-4fbb-4547-c364-43cf1ca3b799"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "highest positive confidence:  {'text': 'Je tady fajn 🚴\\u200d♀️😂', 'probabilities': {'negative': 0.002989264, 'positive': 0.9914219, 'neutral': 0.0055887913}, 'prediction': 'positive', 'prediction_confidence': 0.9914219}\n",
            "highest negative confidence:  {'text': 'Už to do zítra nevydržím, tady to máte.', 'probabilities': {'negative': 0.9691094, 'positive': 0.003227884, 'neutral': 0.02766266}, 'prediction': 'negative', 'prediction_confidence': 0.9691094}\n"
          ]
        }
      ],
      "source": [
        "print(\"highest positive confidence: \", max([i for i in predictions if i[\"prediction\"]==\"positive\"],key=lambda x: x[\"prediction_confidence\"]))\n",
        "print(\"highest negative confidence: \", max([i for i in predictions if i[\"prediction\"]==\"negative\"],key=lambda x: x[\"prediction_confidence\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2:\n",
        "\n",
        "The previous cell contains the posts where the model was the most confident about them being either positive or negative. Is it the same thing as the posts actually being \"the most positive / the most negative\"?\n",
        "\n",
        "What would we need to do to get these answers from the model?\n",
        "\n",
        "#### Author's opinion\n",
        "It's always necessary to keep in mind what the model output actually means.\n",
        "\n",
        "There are two layers to this question. First, the obvious one:\n",
        "\n",
        "Confidence is not intensity. The confidence measures how much the model is sure of its answer. As an example, imagine a post which is strongly negative, but which uses a complicated metaphor or sarcasm to express that. The model would likely have low confidence in its prediction, even though the intensity of the sentiment would be really strong.\n",
        "\n",
        "We can only get the information about the intensity of the sentiment from our three classes: negative, neutral or positive. If we wanted a more detailed resolution, we could e.g. predict 5 classes: strongly negative, negative, neutral, positive and strongly positive. Or we could, instead of classification, train the model to do regression, i.e. predict for each post a single sentiment score, e.g. a real number from -1 to 1. In both of these cases, we would have to retrain the model, because it just wasn't trained to do any of this.\n",
        "\n",
        "The second layer to this question is that the confidence itself means something else to us humans than it does to the model.\n",
        "Confidence of the model is a measure of how far in the feature space is the specific post from the decision boundary.\n",
        "For humans, confidence reflects how likely it is that unknown facts could change their conclusion. A post for which humans have high confidence can have low confidence for the model, and the other way round.\n",
        "\n",
        "-----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_zl2kjdlNdX"
      },
      "source": [
        "Let's look at the overall number of posts that are positive or negative.\n",
        "\n",
        "We will select a threshold of 0.9 as a confidence threshold, above which we trust the model.\n",
        "If the confidence is lower, we leave the post unclassified."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "HKWbPP1cCD6-",
        "outputId": "5a0fb1b3-fa0c-4728-8d54-dba1e79832d4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "threshold=0.9\n",
        "only_confident=[i[\"prediction\"] for i in predictions if i[\"prediction_confidence\"]>threshold]\n",
        "labels,counts=np.unique(only_confident,return_counts=True)\n",
        "counts=list(counts)\n",
        "labels=list(labels)\n",
        "counts+=[len(predictions)-len(only_confident)]\n",
        "labels+=[\"unclassified\"]\n",
        "plt.ylabel(\"counts\")\n",
        "plt.title(name)\n",
        "plt.bar(labels,counts)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl0V-U7RlidI"
      },
      "source": [
        "## Question 3\n",
        "What does this plot say?\n",
        "Does it match your expectations?\n",
        "Is the model good enough for us to make conclusions about the author of these posts?\n",
        "\n",
        "#### Author's opinion\n",
        "The most of the posts were classified as neutral, and the second most likely outcome was unclassified due to low model confidence.\n",
        "\n",
        "There are again two things to keep in mind here.\n",
        "First, our confidence threshold was selected without any rigorous process. Maybe it was set too high, or too low. If we did perform an error analysis as suggested in the Author's Opinion on Question 2, we should be able to tell whether we could trust these results a little better.\n",
        "\n",
        "However, without the error analysis, these results are untrustworthy.\n",
        "What if all the unclassified results were instead positive? Or negative? Although this is unlikely, the overall picture would be very different.\n",
        "\n",
        "Secondly, there is a problem of interpretation of the labels. What does \"positive\" even mean in this context? Is it hopeful and optimistic, or just \"not vulgar and hateful\"? This is not a property of the model, but of the dataset it was trained on.\n",
        "We can get a feel for this interpretation by looking at the original dataset on which the model was trained. Some feeling for this can also be gained from the error analysis itself: if the model usually classifies weakly positive posts as neutral, maybe the annotators of the training data just had a higher threshold for considering something positive. This is something to always keep in mind when interpreting the model results - the meaning of the labels can be different than we expect.\n",
        "\n",
        "From this and the previous questions, we see that even though our model was already trained, it is still useful to label a small test dataset ourselves, and perform a thorough model evaluation to ensure that our predictions are trustworthy and interpreted correctly.\n",
        "\n",
        "## Excercise 2\n",
        "In the next cell, see the results for another politician of your choice. Are they different?\n",
        "Post your results (plots, not texts) into the gallery with the name of the politician."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UctFch8-EXAz",
        "outputId": "18334255-ac79-4ccc-9b9f-ebce970f6a64"
      },
      "outputs": [],
      "source": [
        "name=\"jozko-mrkvicka\"\n",
        "texts=get_texts(name)\n",
        "predictions=list(tqdm(get_predictions(texts),total=len(texts),desc=\"processing texts\"))\n",
        "\n",
        "sb.displot(pd.DataFrame(predictions),x=\"prediction_confidence\",col=\"prediction\")\n",
        "plt.show()\n",
        "\n",
        "threshold=0.9\n",
        "only_confident=[i[\"prediction\"] for i in predictions if i[\"prediction_confidence\"]>threshold]\n",
        "labels,counts=np.unique(only_confident,return_counts=True)\n",
        "counts=list(counts)\n",
        "labels=list(labels)\n",
        "counts+=[len(predictions)-len(only_confident)]\n",
        "labels+=[\"unclassified\"]\n",
        "plt.ylabel(\"counts\")\n",
        "plt.title(name)\n",
        "plt.bar(labels,counts)\n",
        "plt.show()\n",
        "\n",
        "print(\"highest positive confidence: \", max([i for i in predictions if i[\"prediction\"]==\"positive\"],key=lambda x: x[\"prediction_confidence\"]))\n",
        "print(\"highest negative confidence: \", max([i for i in predictions if i[\"prediction\"]==\"negative\"],key=lambda x: x[\"prediction_confidence\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appendix: Transfer learning\n",
        "In the discussion of the previous questions, we encountered a few cases in which it would be nice to have a model that does something slightly different than the model we already have.\n",
        "\n",
        "E.g. we might want to predict a positivity score with higher resolution than just negative, neutral and positive. Or, let's say we wouldn't want to classify the results as \"negative\", \"neutral\" and \"positive\", but as \"polarizing\", \"neutral\" and \"reconciliatory\". Another possibility is that we would find out in our model evaluation that the performance on our specific dataset is low, and we would like to improve it.\n",
        "\n",
        "In all of these cases we could make use of the so-called transfer learning.\n",
        "\n",
        "As is, the model obviously understands something about the czech language. It is assumed that in general, neural networks detect simple features in the first layers (in this case, the simplest being position and presence of words), and as we move into the deeper layers of the networks, the features are becoming more abstract and general (e.g. which word is the subject of the sentence). In the last layers of the model, these general features are combined to make a prediction on our specific task. If we wanted a new model for a similar task, we could leverage the general knowledge about the czech language that should be contained in the first layers of the model.\n",
        "\n",
        "We have two options how to do that:\n",
        "\n",
        "## Finetuning\n",
        "We cut off the last few layers of the model and replace them with the layers that better suit our new needs (e.g. we could change how many output classes is recognized by the output layer). If the task doesn't require different architecture, we can skip this step and just keep the network as is.\n",
        "\n",
        "Then we train the whole model again on a task specific dataset. This new dataset doesn't have to be as large as the original training dataset, because the model doesn't need to learn the rules of the czech language from it (the model already knows them), it only needs to learn how to perform the new specific task. It is likely that if we would do this, the layers that would change the most would be the last layers of the network. It is important to note that this approach doesn't rely on the model having general knowledge somewhere in its inner layers, it's just an advantage. If it is necessary for the training, weights in all of the layers can change. Because of this, this approach guarantees good performance. However, the disadvantage is we need to train all the parameters, which takes time.\n",
        "\n",
        "Positives:\n",
        " - best performance\n",
        "Negatives:\n",
        " - the model takes long to train\n",
        "\n",
        "## Embeddings\n",
        "We cut off the last few layers of the model, so that one of the hidden layers of the model is now an output layer. We feed or data into it and save the outputs. We then use these outputs as training data to train a smaller model to perform our task.\n",
        "\n",
        "The output of some internal hidden layer of the network we just mentioned is usually called an \"embedding\". Since internal layers are usually large, it often has many dimensions (e.g. for transformers it could be 256 or 512). The hope is that this embedding reflects all the relevant features of the data, but is not yet specific to our task.\n",
        "\n",
        "In theory, this is equivalent to finetuning the model, but forbidding the training of the first few layers. However, doing it this way is more practical and faster: the embeddings can be calculated for multiple datapoints in parallel, we don't have to recalculate them multiple times, and we can also use them for other tasks, unrelated to the training.\n",
        "\n",
        "Negatives:\n",
        " - likely not as good as retraining the whole model\n",
        "\n",
        "Positives:\n",
        " - faster training\n",
        " - other good uses of embeddings (e.g. you can search for similar records in the dataset by comparing their embeddings) \n",
        "\n",
        "This article contains a good discussion on this topic in the context of transformers: https://towardsdatascience.com/what-exactly-happens-when-we-fine-tune-bert-f5dc32885d76.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "COdcar5UEot5",
        "KbEbmQinG2OY",
        "2TIoBmL3G72S",
        "zXDOi4ZOLTay"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
